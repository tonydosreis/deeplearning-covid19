{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Deep learning tests.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"skiZfMARCKTK"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"SE0wV5sqCKTM","executionInfo":{"status":"ok","timestamp":1604867839634,"user_tz":-60,"elapsed":37025,"user":{"displayName":"Tony Reis","photoUrl":"https://lh4.googleusercontent.com/-zDfd-tZG1tk/AAAAAAAAAAI/AAAAAAAAAvY/sZbzeubUoYA/s64/photo.jpg","userId":"03434761054634520637"}},"outputId":"fb394827-0c25-4790-d8d3-b7f5656594f5","colab":{"base_uri":"https://localhost:8080/"}},"source":["#to be able to acess google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#premade modules\n","get_ipython().system('pip install pytorch-gradcam')\n","\n","import cv2\n","import random\n","import PIL\n","from torchvision.utils import make_grid, save_image\n","from gradcam.utils import visualize_cam\n","from gradcam import GradCAM, GradCAMpp\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import transforms\n","import torchvision.models as models\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import time\n","import sklearn.metrics\n","from itertools import *"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting pytorch-gradcam\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/0a/55251f7cbea464581c6fb831813d38a41fdeb78f3dd8193522248cb98744/pytorch-gradcam-0.2.1.tar.gz (6.0MB)\n","\u001b[K     |████████████████████████████████| 6.0MB 4.7MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (4.1.2.30)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (1.18.5)\n","Building wheels for collected packages: pytorch-gradcam\n","  Building wheel for pytorch-gradcam (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch-gradcam: filename=pytorch_gradcam-0.2.1-cp36-none-any.whl size=5270 sha256=0ba1b2d6ae8cd28a61381b212d2ca6e1439d801d26ef49e26301f43adc5ac836\n","  Stored in directory: /root/.cache/pip/wheels/e8/1e/35/d24150a078a90ce0ad093586814d4665e945466baa89907300\n","Successfully built pytorch-gradcam\n","Installing collected packages: pytorch-gradcam\n","Successfully installed pytorch-gradcam-0.2.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1g-r2qmlhSxe"},"source":["Paths"]},{"cell_type":"code","metadata":{"id":"r5KNeQp1Z60q","executionInfo":{"status":"ok","timestamp":1604867839644,"user_tz":-60,"elapsed":9461,"user":{"displayName":"Tony Reis","photoUrl":"https://lh4.googleusercontent.com/-zDfd-tZG1tk/AAAAAAAAAAI/AAAAAAAAAvY/sZbzeubUoYA/s64/photo.jpg","userId":"03434761054634520637"}}},"source":["#these are the only global variables\n","base_dir = \"/content/drive/My Drive/Colab Notebooks\"\n","imgs_dir = os.path.join(base_dir, \"Images\")\n","imgs_COVID_dir = os.path.join(imgs_dir, \"CT_COVID\")\n","imgs_NonCOVID_dir = os.path.join(imgs_dir, \"CT_NonCOVID\")\n","data_split_dir = os.path.join(base_dir,\"Data-split\")\n","ds_COVID_dir = os.path.join(data_split_dir,\"COVID\")\n","ds_NonCOVID_dir = os.path.join(data_split_dir,\"NonCOVID\")\n","code_dir = os.path.join(base_dir, \"Code\")\n","Buffer_dir = os.path.join(code_dir, \"Buffer\")"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_ROJxI3CKTS"},"source":["Data preparation"]},{"cell_type":"code","metadata":{"id":"RAnhKMwqCKTU","executionInfo":{"status":"ok","timestamp":1604868402011,"user_tz":-60,"elapsed":552,"user":{"displayName":"Tony Reis","photoUrl":"https://lh4.googleusercontent.com/-zDfd-tZG1tk/AAAAAAAAAAI/AAAAAAAAAvY/sZbzeubUoYA/s64/photo.jpg","userId":"03434761054634520637"}}},"source":["normalize_grayscale = transforms.Normalize(mean=[0.449], std=[0.226])\n","normalize_rgb = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","def getTrainTransform_rgb():\n","\n","    trainTransform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize_rgb\n","    ])\n","\n","    return trainTransform, \"trainTransform RGB\"\n","\n","def getValTransformCrop():\n","\n","    trainTransform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize_rgb\n","    ])\n","\n","    return trainTransform, \"trainTransform RGB\"\n","\n","def getTrainTransform_grayscale():\n","\n","    trainTransform = transforms.Compose([\n","        #transforms.Resize( (224,224) ),\n","        transforms.Resize(256),\n","        transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize_grayscale\n","    ])\n","\n","    return trainTransform, \"trainTransform grayscale\"\n","\n","def getTrainTransform_newSize():\n","\n","    trainTransform = transforms.Compose([\n","        transforms.Resize( (407,287) ),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        normalize_rgb\n","    ])\n","\n","def getValTransform_rgb():\n","\n","    valTransform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        normalize_rgb\n","    ])\n","\n","    return valTransform, \"valTransform RGB\"\n","\n","def getValTransform_grayscale():\n","\n","  valTransform = transforms.Compose([\n","      transforms.Resize(256),\n","      transforms.CenterCrop(224),\n","      transforms.ToTensor(),\n","      normalize_grayscale\n","  ])\n","\n","  return valTransform, \"valTransform grayscale\""],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cQYxWdmCKTg","executionInfo":{"status":"ok","timestamp":1604867859199,"user_tz":-60,"elapsed":655,"user":{"displayName":"Tony Reis","photoUrl":"https://lh4.googleusercontent.com/-zDfd-tZG1tk/AAAAAAAAAAI/AAAAAAAAAvY/sZbzeubUoYA/s64/photo.jpg","userId":"03434761054634520637"}}},"source":["def read_txt(txt_path):\n","    with open(txt_path) as f:\n","        lines = f.readlines()\n","    txt_data = [line.strip() for line in lines]\n","    return txt_data\n","\n","class CovidCTDataset(Dataset):\n","    def __init__(self, txt_COVID, txt_NonCOVID, transform=None, color = \"RGB\", n_crops = 1):\n","\n","        self.color = color\n","        self.n_crops = n_crops\n","        self.txt_path = [txt_COVID,txt_NonCOVID]\n","        self.classes = ['CT_COVID', 'CT_NonCOVID']\n","        self.num_cls = len(self.classes)\n","        \n","        #List of lists. Each list contains the full path of an image and 0 if Covid, 1 if non Covid\n","        self.img_list = []\n","        \n","        for c in range(self.num_cls):\n","            cls_list = [[item, c] for item in read_txt(self.txt_path[c])]\n","            self.img_list += cls_list\n","            \n","        self.transform = transform\n","\n","        \n","    #aparrently __len__() and __getitem__() must be implemented for map-style datasets    \n","    def __len__(self):\n","        return len(self.img_list)\n","\n","    def __getitem__(self, idx):\n","\n","      img_path = self.img_list[idx][0]\n","\n","      if (self.color == \"grayscale\"):\n","        original_img = Image.open(img_path).convert('L')\n","      else:\n","        original_img = Image.open(img_path).convert('RGB')\n","\n","      crops = []\n","      for i in range(self.n_crops):\n","        #Appends a different crop of the same image, as long as there is randomness in the transform\n","        crops.append(self.transform(original_img))\n","\n","      sample = {'img': crops,\n","                'label': int(self.img_list[idx][1])}\n","      \n","      #Returns dictionary that contains image (torch tensor, because of transform) and the label (0,1)\n","      return sample\n","\n","def getDataloaders(trainTransform, valTransform, batchsize, color, n_crops):\n","\n","    trainset = CovidCTDataset(txt_COVID = os.path.join(ds_COVID_dir, \"trainCT_COVID.txt\"),\n","                              txt_NonCOVID = os.path.join(ds_NonCOVID_dir, \"trainCT_NonCOVID.txt\"),\n","                              transform = trainTransform, color = color)\n","\n","    testset = CovidCTDataset(txt_COVID = os.path.join(ds_COVID_dir, \"testCT_COVID.txt\"),\n","                             txt_NonCOVID = os.path.join(ds_NonCOVID_dir, \"testCT_NonCOVID.txt\"),\n","                             transform= valTransform, color = color, n_crops = n_crops)\n","\n","    #I wonder what difference it makes to change drop_last.\n","    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n","    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)\n","\n","    return train_loader, test_loader"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ANcUbqvfz7j"},"source":["K-fold validation"]},{"cell_type":"code","metadata":{"id":"jJ5MXkQv6h8F","executionInfo":{"status":"ok","timestamp":1604867880264,"user_tz":-60,"elapsed":1249,"user":{"displayName":"Tony Reis","photoUrl":"https://lh4.googleusercontent.com/-zDfd-tZG1tk/AAAAAAAAAAI/AAAAAAAAAvY/sZbzeubUoYA/s64/photo.jpg","userId":"03434761054634520637"}}},"source":["class Patient:\n","    def __init__(self, ID, imgs, covid):\n","        self.ID = ID\n","        self.imgs = imgs\n","        self.covid = covid\n","        \n","    def __len__(self):\n","        return len(self.imgs)\n","    \n","    def __str__(self):\n","        return f\"Patient ID: {self.ID}\\nImages: {self.imgs}\"\n","\n","class Partition:\n","    def __init__(self,size):\n","        self.size = size\n","        self.patients = []\n","        self.occupied = 0\n","        \n","    def add(self, patient):\n","        self.patients.append(patient)\n","        self.occupied += len(patient)\n","    \n","    def freeSpace(self):\n","        return (self.size - self.occupied)\n","    \n","    def __len__(self):\n","        return self.size\n","    \n","    def __str__(self):\n","        string = f\"Size: {self.size}\\n\"\n","        string += f\"Occupied: {self.occupied}\\n\"\n","        string += f\"Patients: {self.patients}\"\n","        return string\n","\n","class RandomPartitions:\n","    def __init__(self, patients, nPartitions, proportions = []):\n","        \n","        self.partitionList = []\n","        \n","        #Calculates total number of images\n","        nImages = np.sum([ len(patient) for patient in patients ])\n","        \n","        #equal proportions\n","        if len(proportions) == 0:\n","            proportions = (1/nPartitions) * np.ones(nPartitions)\n","        else:\n","            proportions = np.array(proportions)\n","\n","        #caculates partitions size and converts to int\n","        partitionSizes = (np.round(nImages*proportions)).astype(int)\n","        #makes sure that sum of all partition sizes is equal to number of images\n","        partitionSizes[-1] = nImages - partitionSizes[0:nPartitions-1].sum()\n","\n","        #Adds to the partition list, a partition for every partition size\n","        for partitionSize in partitionSizes:\n","            self.partitionList.append(Partition(partitionSize))\n","        \n","        priorityQueue = sorted(patients,key=len).copy()        \n","\n","        #A number for each partition\n","        choices = np.arange(nPartitions)\n","        \n","        #while there is something to allocate\n","        while(len(priorityQueue) > 0):\n","            \n","            #extracts patient from priority Queue\n","            patient = priorityQueue.pop()\n","            \n","            #probability of being allocated to each partition\n","            p = np.array([partition.freeSpace() for partition in self.partitionList])\n","            p = p/np.sum(p)\n","            \n","            #randomly chooses a partition\n","            partition = np.random.choice(choices,p = p)\n","            \n","            #puts patient into partition\n","            self.partitionList[partition].add(patient)\n","    \n","    def toLists(self):\n","        #gets a list of lists from a list of partition objets\n","        partitions = [partition.patients for partition in self.partitionList]\n","        \n","        return partitions\n","            \n","    def __len__(self):\n","        return len(self.partitionList)\n","\n","#Organizes images by patients\n","def loadPatients():\n","    \n","    #Relevant paths\n","    info_dir = os.path.join(base_dir, \"Info\")\n","    COVID_info = os.path.join(info_dir,\"COVID-CT-MetaInfo.csv\")\n","    NonCOVID_info = os.path.join(info_dir,\"NonCOVID-CT-MetaInfo.csv\")\n","\n","    COVID_dict = dict() #key is the patient and the value is a list of image paths\n","    NonCOVID_dict = dict()\n","    \n","    #COVID\n","    file = open(COVID_info,\"r\")\n","\n","    #skips first line (its useless for us)\n","    file.readline()\n","\n","    #Goes through evey line\n","    for line in file:\n","\n","        #Retrieves the information we need\n","        image_name, patient = line.split(\",\")[0:2]\n","\n","        image_path = os.path.join(imgs_COVID_dir, image_name)\n","\n","        #patient already exists\n","        if(patient in COVID_dict):\n","            COVID_dict[patient].append(image_path)\n","\n","        #patient does not exist\n","        else:\n","            COVID_dict[patient] = [image_path]\n","\n","    file.close()\n","\n","    #NonCOVID\n","    file = open(NonCOVID_info,\"r\")\n","\n","    #skips first line\n","    file.readline()\n","\n","    #Goes through evey line\n","    for line in file:\n","\n","        #Retrieves the information we need\n","        image_name, patient = line.split(\",\")[1:3]\n","        patient = patient.rstrip(\"\\n\")\n","\n","        image_path = os.path.join(imgs_NonCOVID_dir, image_name)\n","\n","        #Patient already exists\n","        if(patient in NonCOVID_dict):\n","            NonCOVID_dict[patient].append(image_path)\n","\n","        #Patient does not exist\n","        else:\n","            NonCOVID_dict[patient] = [image_path]\n","\n","    file.close()\n","\n","    #Converts dictionnary into list of Patient objects\n","    patients = []\n","    for key,value in COVID_dict.items():\n","        patients.append( Patient(key,value,1) )\n","        \n","    for key,value in NonCOVID_dict.items():\n","        patients.append( Patient(key,value,0) )\n","\n","    #Returns a list of patient objects\n","    return patients\n","\n","#converts lists of lists of an element into a list of all elements\n","def unpack(outerList):\n","    newList = []\n","    for inerList in outerList:\n","        newList += inerList\n","    return newList\n","\n","class CovidCTDataset_patients(Dataset):\n","  def __init__(self, patients, transform, color = \"RGB\", n_crops = 1):\n","      \n","    self.color = color\n","    self.n_crops = n_crops\n","    self.img_list = []\n","    self.transform = transform\n","    \n","    #unpacks list patients\n","    for patient in patients: #for every patient\n","        for img in patient.imgs: #for every image of the patient\n","            self.img_list.append( (img , patient.covid) ) #adds (image,label) to img_list \n","      \n","  def __len__(self):\n","    return len(self.img_list)\n","\n","  def __getitem__(self, idx):\n","\n","    img_path = self.img_list[idx][0]\n","    if (self.color == \"grayscale\"):\n","      original_img = Image.open(img_path).convert('L')\n","    else:\n","      original_img = Image.open(img_path).convert('RGB')\n","\n","    crops = []\n","    for i in range(self.n_crops):\n","      #Appends a different crop of the same image, as long as there is randomness in the transform\n","      crops.append(self.transform(original_img))\n","\n","    sample = {'img': crops,\n","              'label': int(self.img_list[idx][1])}\n","    \n","    return sample\n","\n","#produces dataloaders for simple hold out validation (train + test)\n","def holdOut_getLoaders(train_test_split, trainTransform, testTransform, batchsize, color, n_crops):\n","\n","  #gets a list of all the patients from the data\n","  allPatients = loadPatients()\n","  \n","  train_test_proportions = [train_test_split, 1 - train_test_split]\n","\n","  #train+val and test split\n","  train_list, test_list = RandomPartitions(allPatients, 2, train_test_proportions).toLists()\n","\n","  train_set = CovidCTDataset_patients(train_list, transform = trainTransform, color = color)\n","  test_set = CovidCTDataset_patients(test_list, transform = testTransform, color = color, n_crops = n_crops)\n","\n","  train_loader = DataLoader(train_set, batch_size=batchsize, drop_last=False, shuffle=True)\n","  test_loader = DataLoader(test_set, batch_size=batchsize, drop_last=False, shuffle=False)\n","\n","  return train_loader, test_loader\n","\n","def kfold_getLists(trainVal_test_split,k):\n","\n","  #gets a list of all the patients from the data\n","  allPatients = loadPatients()\n","  \n","  trainVal_test_proportions = [trainVal_test_split, 1 - trainVal_test_split]\n","\n","  #train+val and test split\n","  trainVal_list, test_list = RandomPartitions(allPatients, 2, trainVal_test_proportions).toLists()\n","\n","  #devides trainVal_list into k equal partitions\n","  kfold_partitions = RandomPartitions(trainVal_list,k).toLists()\n","\n","  #creates the different combinations of train and validation sets\n","  \n","  #each element is (train_list, val_list)\n","  trainAndVal_lists = []\n","  \n","  for i, partition in enumerate(kfold_partitions):\n","\n","    val_list = partition\n","    train_list = unpack( kfold_partitions[:i] + kfold_partitions[i+1:] )\n","\n","    trainAndVal_lists.append( (train_list, val_list) )\n","\n","  return trainAndVal_lists, test_list\n","\n","def kfold_lists2loaders(trainAndVal_lists, test_list, train_transform, val_transform, batchsize, color,  n_crops = 1):\n","\n","    test_set = CovidCTDataset_patients(test_list, transform = val_transform,color = color)\n","    test_loader = DataLoader(test_set, batch_size=batchsize, drop_last=False, shuffle=False)\n","\n","    trainAndValLoaders = []\n","    \n","    for  (train_list, val_list) in trainAndVal_lists:\n","\n","        train_set = CovidCTDataset_patients(train_list, train_transform,color)\n","        val_set = CovidCTDataset_patients(val_list, val_transform, color, n_crops)\n","\n","        train_loader = DataLoader(train_set, batch_size=batchsize, drop_last=False, shuffle=True)\n","        val_loader = DataLoader(val_set, batch_size=batchsize, drop_last=False, shuffle=False)\n","\n","        trainAndValLoaders.append( (train_loader, val_loader) )\n","\n","    return trainAndValLoaders, test_loader\n","\n","#combines kfold_getLists and kfold_lists2loaders\n","def kfold_getDataloaders(train_transform, val_transform, trainVal_test_split, k, batchsize, color, n_crops = 1):\n","    \n","    trainAndVal_lists, test_list = kfold_getLists(trainVal_test_split,k)\n","    trainAndValLoaders, test_loader = kfold_lists2loaders(trainAndVal_lists, test_list, train_transform, val_transform, batchsize, \n","                                                          color, n_crops = 1)\n","\n","    return trainAndValLoaders, test_loader\n","\n","def kfold_completeTrain(device, trainAndValLoaders, getModel, getOptimizer, epochs, learning_rate, weight_decay = 0, momentum = 0):\n","\n","    kfold_metrics = []\n","\n","    for i, (train_loader, val_loader) in enumerate(trainAndValLoaders):\n","\n","        print(f\"\\nFold: {i}\\n\")\n","\n","        model = 0\n","        \n","        del model\n","        \n","        #getModel returns model and modelname, we only want the model here\n","        model = getModel()[0]\n","\n","        fold_metrics = completeTrain(device, model, getOptimizer, train_loader, val_loader, epochs, learning_rate, weight_decay)\n","\n","        kfold_metrics.append(fold_metrics)\n","    \n","    return getAverageMetrics_kfold(kfold_metrics)\n","\n","def getAverageMetrics_kfold(kfold_metrics):\n","    \n","    epochs = len(kfold_metrics[0][1])\n","    k = len(kfold_metrics)\n","\n","    CM_train_average = np.zeros( (epochs,2,2), dtype = np.float32 )\n","    CM_val_average = np.zeros( (epochs,2,2 ), dtype = np.float32 )\n","    LOSS_train_average = np.zeros(epochs, dtype = np.float32)\n","    LOSS_val_average = np.zeros(epochs, dtype = np.float32)\n","\n","    for CM_train, LOSS_train, CM_val, LOSS_val in kfold_metrics:\n","\n","        CM_train_average += CM_train\n","        CM_val_average += CM_val\n","        LOSS_train_average += LOSS_train\n","        LOSS_val_average += LOSS_val\n","\n","    CM_train_average /= k\n","    CM_val_average /= k\n","    LOSS_train_average /= k\n","    LOSS_val_average /= k\n","\n","    return CM_train_average, LOSS_train_average, CM_val_average, LOSS_val_average"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"byeuB9Oo6Usu"},"source":["Models"]},{"cell_type":"code","metadata":{"id":"xxLo2gbHaQWI"},"source":["def setUpGetResnet50(classifier_widths, pretrained = True, unfrozenLayers = 0, classifier_input_size = 2048):\n","\n","  class Classifier(nn.Module):\n","    def __init__(self, classifier_widths):\n","      super(Classifier, self).__init__()\n","      #classifier module\n","      self.layers = nn.Sequential()\n","\n","      #hidden layers\n","      for i in range(1,len(classifier_widths)):\n","        self.layers.add_module(f\"{ 3*(i-1) }\",nn.Linear(classifier_widths[i-1],classifier_widths[i]))\n","        self.layers.add_module(f\"{ 3*(i-1) + 1 }\",nn.ReLU(True))\n","        self.layers.add_module(f\"{ 3*(i-1) + 2 }\",nn.Dropout())\n","\n","      #output layer\n","      self.layers.add_module(f\"{3*( len(classifier_widths) - 1) }\",nn.Linear(classifier_widths[-1],2))\n","\n","    def forward(self, x):        \n","        return self.layers(x)\n","\n","  #for first layer\n","  classifier_widths.insert(0, classifier_input_size)\n","\n","  name = f\"resnet-50, pretrained = {pretrained}, number of unfrozen layers = {unfrozenLayers}, width of classifier layers = {classifier_widths}\"\n","\n","  def getResnet50():\n","    \n","    model = models.resnet50(pretrained = pretrained)\n","\n","    if pretrained:\n","      #freeze all parameters\n","      for params in model.parameters():\n","        params.requires_grad = False\n","      \n","      #unfreeze some from the end\n","      parameters = list(model.parameters())\n","      parameters.reverse()\n","      for i in range(2*unfrozenLayers):\n","        parameters[i].requires_grad = True\n","\n","    model.fc = Classifier(classifier_widths)\n","\n","    return model, name\n","\n","  return getResnet50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjDE_9CE8jFM"},"source":["def setUpGetDensenet169(classifier_widths, pretrained = True, unfrozenLayers = 0, classifier_input_size = 1664):\n","  \n","  class Classifier(nn.Module):\n","    def __init__(self, classifier_widths):\n","      super(Classifier, self).__init__()\n","      #classifier module\n","      self.layers = nn.Sequential()\n","\n","      #hidden layers\n","      for i in range(1,len(classifier_widths)):\n","        self.layers.add_module(f\"{ 3*(i-1) }\",nn.Linear(classifier_widths[i-1],classifier_widths[i]))\n","        self.layers.add_module(f\"{ 3*(i-1) + 1 }\",nn.ReLU(True))\n","        self.layers.add_module(f\"{ 3*(i-1) + 2 }\",nn.Dropout())\n","\n","      #output layer\n","      self.layers.add_module(f\"{3*( len(classifier_widths) - 1) }\",nn.Linear(classifier_widths[-1],2))\n","\n","    def forward(self, x):        \n","        return self.layers(x)\n","\n","  #for first layer\n","  classifier_widths.insert(0, classifier_input_size)\n","\n","  name = f\"densenet, pretrained = {pretrained}, number of unfrozen layers = {unfrozenLayers}, width of classifier layers = {classifier_widths}\"\n","\n","  def getDensenet169():\n","    \n","    model = models.densenet169(pretrained = pretrained)\n","\n","    if pretrained:\n","      for params in model.parameters():\n","        params.requires_grad = False\n","      \n","      parameters = list(model.parameters())\n","      parameters.reverse()\n","      for i in range(2*unfrozenLayers):\n","        parameters[i].requires_grad = True\n","\n","    model.classifier = Classifier(classifier_widths)\n","\n","    return model, name\n","\n","  return getDensenet169"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmWa9zIzH2XY"},"source":["def setUpGetVgg16(classifier_widths, unfrozenLayers = 0, classifier_input_size = 25088):\n","\n","  class nothing(nn.Module):\n","      def __init__(self):\n","          super(nothing, self).__init__()        \n","      def forward(self, x):        \n","          return x\n","\n","  class Classifier(nn.Module):\n","    def __init__(self, classifier_widths):\n","      super(Classifier, self).__init__()\n","      #classifier module\n","      self.layers = nn.Sequential()\n","\n","      #hidden layers\n","      for i in range(1,len(classifier_widths)):\n","        self.layers.add_module(f\"{ 3*(i-1) }\",nn.Linear(classifier_widths[i-1],classifier_widths[i]))\n","        self.layers.add_module(f\"{ 3*(i-1) + 1 }\",nn.ReLU(True))\n","        self.layers.add_module(f\"{ 3*(i-1) + 2 }\",nn.Dropout())\n","\n","      #output layer\n","      self.layers.add_module(f\"{3*( len(classifier_widths) - 1) }\",nn.Linear(classifier_widths[-1],2))\n","\n","    def forward(self, x):        \n","        return self.layers(x)\n","\n","  #for first layer\n","  classifier_widths.insert(0,classifier_input_size)\n","\n","  #this needs to be adepted as a function of the paramerters\n","  modelname = f\"vgg16 pretained, number of unfrozen layers = {unfrozenLayers}, width of classifier layers = {classifier_widths}\"\n","\n","  def getVgg16():\n","\n","    model = models.vgg16(pretrained=True)\n","\n","    #Freezes every layer\n","    for i,param in enumerate(model.parameters()):\n","        #there is a *2 because every layer has two parameter tensors, one for weights and one for biases\n","        if i > (25 - 2*unfrozenLayers):\n","          break\n","        param.requires_grad = False\n","\n","    #the average pool only does something if the input image size is different than 224x224\n","    model.avgpool = nothing()\n","\n","    model.classifier = Classifier(classifier_widths)\n","\n","    return model, modelname\n","  \n","  return getVgg16\n","\n","def genGetVgg16DiffClass(base):\n","    \n","  widths = [ [ int(base/2) ], [base], [2*base] ]\n","  \n","  for w in widths.copy():\n","    widths.append( w + [ int(w[0]/2) ] )\n","      \n","  for w in islice(widths.copy(), 3, None):\n","    widths.append( w + [ int(w[1]/2) ])\n","\n","  getVgg16s = []\n","  for width in widths:\n","    getVgg16s.append( setUpGetVgg16(classifier_widths=width) )\n","  \n","  return getVgg16s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nixFY5FAbxbC"},"source":["def setUpGetVgg16_grayscale(unfrozenLayers = 0, classifier_input_size = 25088, classifier_widths = [1000], convert_type = \"average\"):\n","\n","  class nothing(nn.Module):\n","      def __init__(self):\n","          super(nothing, self).__init__()        \n","      def forward(self, x):        \n","          return x\n","\n","  class Classifier(nn.Module):\n","    def __init__(self, classifier_widths):\n","      super(Classifier, self).__init__()\n","      #classifier module\n","      self.layers = nn.Sequential()\n","\n","      #hidden layers\n","      for i in range(1,len(classifier_widths)):\n","        self.layers.add_module(f\"{ 3*(i-1) }\",nn.Linear(classifier_widths[i-1],classifier_widths[i]))\n","        self.layers.add_module(f\"{ 3*(i-1) + 1 }\",nn.ReLU(True))\n","        self.layers.add_module(f\"{ 3*(i-1) + 2 }\",nn.Dropout())\n","\n","      #output layer\n","      self.layers.add_module(f\"{3*( len(classifier_widths) - 1) }\",nn.Linear(classifier_widths[-1],2))\n","\n","    def forward(self, x):        \n","        return self.layers(x)\n","\n","  #for first layer\n","  classifier_widths.insert(0,classifier_input_size)\n","\n","  #this needs to be adepted as a function of the paramerters\n","  modelname = f\"vgg16 pretained, number of unfrozen layers = {unfrozenLayers}, width of classifier layers = {classifier_widths}, convert = {convert_type}\"\n","\n","  def getVgg16():\n","\n","    model = get_vgg16_grayscale(convert_type)\n","\n","    #Freezes every layer\n","    for i,param in enumerate(model.parameters()):\n","        #there is a *2 because every layer has two parameter tensors, one for weights and one for biases\n","        if i > (25 - 2*unfrozenLayers):\n","          break\n","        param.requires_grad = False\n","\n","    #the average pool only does something if the input image size is different than 224x224\n","    model.avgpool = nothing()\n","\n","    model.classifier = Classifier(classifier_widths)\n","\n","    return model, modelname\n","  \n","  return getVgg16\n","\n","def get_vgg16_grayscale(convert_type):\n","  model = models.vgg16(pretrained=True)\n","\n","  #get old first conv layer\n","  old_input_layer = model.features[0]\n","\n","  #gets old first conv layer weights\n","  rgb_weights = old_input_layer.weight.data\n","\n","  if (convert_type == \"luma\"):\n","    rgb_weights[:,0,:,:] =  0.897*rgb_weights[:,0,:,:]\n","    rgb_weights[:,1,:,:] =  1.761*rgb_weights[:,1,:,:]\n","    rgb_weights[:,2,:,:] =  0.342*rgb_weights[:,2,:,:]\n","\n","  #calculates new grayscale weights\n","  grayscale_weights = rgb_weights.sum(dim=1)\n","  #adds a dummy dimension\n","  grayscale_weights = grayscale_weights.view(64,1,3,3)\n","\n","  #new conv_layer \n","  new_input_layer = nn.Conv2d(1, 64, (3, 3), stride=(1, 1), padding=(1, 1))\n","  #copys the grayscale weights to new conv layer\n","  new_input_layer.weight.data = grayscale_weights\n","  #copys old bias to new conv layer\n","  new_input_layer.bias = old_input_layer.bias\n","\n","  #replaces old conv layer with new conv layer\n","  model.features[0] = new_input_layer\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYJq8m9x6anO"},"source":["def printOutputShape(model):\n","  patients = loadPatients()\n","  dataset = CovidCTDataset_patients(patients, getTrainTransform_newSize()[0])\n","  loader = DataLoader(dataset, batch_size=1)\n","\n","  batch = iter(loader).next()\n","  input = batch[\"img\"][0]\n","\n","  with torch.no_grad():\n","    output = model(input)\n","\n","  print(output.shape)\n","\n","printOutputShape(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKDQp3Bogeru"},"source":["optimizers"]},{"cell_type":"code","metadata":{"id":"tx5FpUzxQw2F"},"source":["def getRMSprop():\n","    return torch.optim.RMSprop, \"RMSprop\"\n","\n","def getSGD():\n","    return torch.optim.SGD, \"SGD\"\n","\n","def getAdam():\n","    return torch.optim.Adam, \"Adam\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jy7x0FQDCKUz"},"source":["Train and validation functions"]},{"cell_type":"code","metadata":{"id":"FmEoNye4CKU1"},"source":["def train(device, model, dataloader, criterion, optimizer):\n","        \n","    model.train()\n","    \n","    #gets data size\n","    data_size = len(dataloader.dataset)\n","    \n","    #all targets of epoch\n","    epoch_target = torch.zeros(data_size)\n","    #all predictions of epoch\n","    epoch_pred = torch.zeros(data_size)\n","    \n","    epoch_loss = 0\n","    \n","    #Goes through every batch\n","    for i, batch in enumerate(dataloader):\n","                        \n","        #Separates input and output\n","        #I must use [0] because it is always a list, even if there only one element\n","        imgs, target = batch['img'][0].to(device), batch['label'].to(device)\n","        #gets batch size\n","        batch_size = len(target)\n","                        \n","        #Forward pass\n","        output = model(imgs)\n","        \n","        #Calculates loss for batch\n","        loss = criterion(output, target)\n","                        \n","        #Adds batch loss to epoch_loss\n","        epoch_loss += loss.item()*batch_size\n","        \n","        #Calculates gradient (backpropagation)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        #updates weights\n","        optimizer.step()\n","        \n","        #Gets prediction from class scores\n","        pred = output.argmax(dim=1)\n","        \n","        #groups data of all batches\n","        epoch_pred[i*batch_size : (i + 1) * batch_size] = pred\n","        epoch_target[i*batch_size : (i + 1) * batch_size] = target\n","    \n","    #creates confusion matrix\n","    CM = sklearn.metrics.confusion_matrix(epoch_target, epoch_pred)\n","\n","    #loss per sample\n","    mean_epoch_loss = epoch_loss/data_size\n","    \n","    return CM, mean_epoch_loss\n","\n","def val(device, model, dataloader, criterion):\n","            \n","    model.eval()\n","    \n","    #gets size of data\n","    data_size = len(dataloader.dataset)\n","    \n","    #Stores data for the entire epoch\n","    epoch_pred = torch.zeros(data_size)\n","    epoch_target = torch.zeros(data_size)\n","    \n","    #sum of the loss of every batch/not used\n","    epoch_loss = 0\n","     \n","    #Does not keep track of operations for gradient calculation\n","    with torch.no_grad():\n","                        \n","        for i, batch in enumerate(dataloader):\n","            \n","            #Seperates input and output\n","            imgs, target = batch['img'], batch['label'].to(device)\n","            #gets size of batch\n","            batch_size = len(target)\n","\n","            output = torch.zeros( (batch_size,2), device = torch.device(device) )\n","\n","            #goes through every crop         \n","            for crop in imgs:\n","\n","                crop = crop.to(device)\n","                #passes crop though the model\n","                output += model(crop)\n","\n","            #average score of all crops\n","            output = output / len(imgs)\n","            \n","            #sums batch loss to epoch_loss\n","            epoch_loss += criterion(output, target).item()*batch_size\n","            \n","            #Calculates prediction based on class scores\n","            pred = output.argmax(dim=1)\n","            \n","            #groups data of all batches\n","            epoch_target[i*batch_size : (i + 1)*batch_size] = target\n","            epoch_pred[i*batch_size : (i + 1)*batch_size] = pred\n","        \n","        CM = sklearn.metrics.confusion_matrix(epoch_target, epoch_pred)\n","        \n","        #loss per sample\n","        mean_epoch_loss = epoch_loss/data_size\n","    \n","    return CM, mean_epoch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9bVWfupVCKVA"},"source":["Main training loop"]},{"cell_type":"code","metadata":{"id":"xb91CknaCKVB"},"source":["def completeTrain(device, model, getOptimizer, train_loader, val_loader, epochs, learning_rate, weight_decay = 0, momentum = 0):\n","\n","    torch.cuda.empty_cache()\n","    model = model.to(device)\n","\n","    #get optimizer returns a function and a name, here we only want the function. That is why there is [0]\n","    optimizer = getOptimizer()[0](model.parameters(), learning_rate, weight_decay = weight_decay, momentum = momentum)\n","    \n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    LOSS_train, CM_train, LOSS_val, CM_val = [], [], [], []\n","\n","    t_total = 0\n","\n","    for epoch in range(epochs): \n","    \n","        print(f\"Epoch: {epoch:2.0f},\", end = \" \")\n","            \n","        t_start = time.time()\n","\n","        cm_val, loss_val = val(device, model,val_loader,criterion)\n","        \n","        t_end = time.time()\n","        t_val = t_end-t_start\n","        print(f\"Val time: {t_val:5.2f}s\", end = \" \")\n","\n","        t_start = time.time()\n","\n","        cm_train, loss_train = train(device, model, train_loader,criterion, optimizer)\n","\n","        t_end = time.time()\n","        t_train = t_end-t_start\n","        print(f\"Train time: {t_train:5.2f}s\")\n","            \n","        CM_train.append(cm_train)\n","        LOSS_train.append(loss_train)\n","        CM_val.append(cm_val)\n","        LOSS_val.append(loss_val)\n","\n","        t_total += (t_train + t_val)\n","    \n","    print(f\"\\nTotal time: {formatTime(t_total)}\")\n","\n","    return np.array(CM_train), np.array(LOSS_train), np.array(CM_val), np.array(LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuBxYStOvGR7"},"source":["Hyperparameter search"]},{"cell_type":"code","metadata":{"id":"NedoEq5tvEre"},"source":["class hyperparameter():\n","\n","  next_id = 0\n","\n","  def __init__(self, batchsize, getTrainTransform, getValTransform, getModel, getOptimizer, learning_rate, \n","              epochs, weight_decay, momentum, color, n_crops, trainVal_test_split = None, k = None):\n","      \n","    self.id = hyperparameter.next_id\n","    hyperparameter.next_id += 1\n","\n","    self.trainVal_test_split = trainVal_test_split\n","    self.k = k\n","    self.batchsize = batchsize\n","    self.getTrainTransform = getTrainTransform\n","    self.getValTransform = getValTransform\n","    self.getModel = getModel\n","    self.getOptimizer = getOptimizer\n","    self.learning_rate = learning_rate\n","    self.epochs = epochs\n","    self.weight_decay = weight_decay\n","    self.momentum = momentum\n","    self.color = color\n","    self.n_crops = n_crops\n","\n","  def getId(self):\n","      return str(self.id)\n","\n","  def __str__(self):\n","    return f\"Model {self.id}, trainVal_test_split = {self.trainVal_test_split}, k = {self.k}, batchsize = {self.batchsize}, train transform = {self.getTrainTransform()[1]}, val transform = {self.getValTransform()[1]}, model = {self.getModel()[1]}, optimizer = {self.getOptimizer()[1]}, learning rate = {self.learning_rate}, epochs = {self.epochs}, weight decay = {self.weight_decay}, momentum = {self.momentum}, color = {self.color}, n_crops = {self.n_crops}\"\n","\n","def addParameterCombinations(prev_combinations, parameter_list):\n","    \n","    new_combinations = []\n","    \n","    for combination in prev_combinations:\n","        \n","        for parameter in parameter_list:\n","            \n","            new_combination = combination.copy() + [parameter]\n","            new_combinations.append(new_combination)\n","            \n","    return new_combinations\n","\n","def getParamCombinations(parameters):\n","    \n","    for i, parameter in enumerate(parameters):\n","\n","      if (i == 0):\n","        combinations = [ [p] for p in parameter]\n","      else:\n","        combinations = addParameterCombinations(combinations, parameter)\n","\n","    hps = []\n","    for c in combinations:\n","      hps.append( hyperparameter(c[0],c[1],c[2],c[3],c[4],c[5],c[6],c[7],c[8],c[9],c[10]) )\n","\n","    return hps\n","\n","#used as sorting key later on\n","def get2ele(Tuple): #gets second element of a tuple\n","    return Tuple[1]\n","\n","def configs2file(filename, configs):\n","\n","    #creates the file if it does not exist\n","    #erases file and creates new file if already exists\n","    f = open(filename, \"w\")\n","    f.close()\n","\n","    #opens file for appending\n","    f = open(filename, \"a\")\n","\n","    for (name, mcc, acc, time) in configs:\n","        f.write(f\"[mcc {mcc:2.2f}, acc {acc:2.2f}, time {time}] : {name}\\n\\n\")\n","    f.close()\n","\n","def loadNpz(filename):\n","    filename = os.path.join(Buffer_dir, filename + \".npz\")\n","\n","    npzfile = np.load(filename)\n","\n","    CM_train = npzfile[\"CM_train\"]\n","    LOSS_train = npzfile[\"LOSS_train\"]\n","    CM_val = npzfile[\"CM_val\"] \n","    LOSS_val = npzfile[\"LOSS_val\"]\n","\n","    return CM_train, LOSS_train, CM_val, LOSS_val\n","\n","def hyperparameterSetSearch(hps):\n","\n","    dev = \"cuda\"\n","    trainVal_test_split = 0.9\n","    k = 5\n","\n","    trainAndVal_lists, test_list = kfold_getLists(trainVal_test_split = trainVal_test_split, k = k)\n","\n","    configs = []\n","\n","    for hp in hps:\n","\n","        hp.trainVal_test_split = trainVal_test_split\n","        hp.k = k\n","\n","        t_start = time.time()\n","\n","        #I dont need the test loader here, that is why there is a [0] at the end\n","        trainAndValLoaders = kfold_lists2loaders(trainAndVal_lists, test_list, hp.getTrainTransform()[0], hp.getValTransform()[0], hp.batchsize, hp.color, hp.n_crops )[0]\n","\n","        CM_train, LOSS_train, CM_val, LOSS_val = kfold_completeTrain(dev, trainAndValLoaders, hp.getModel, hp.getOptimizer, hp.epochs, hp.learning_rate, hp.weight_decay, hp.momentum)\n","\n","        #creates a path to save metrics for current combination\n","        filename = os.path.join(Buffer_dir, f\"Model {hp.getId()}\" )\n","\n","        #saves these metrics to a file\n","        np.savez(filename, CM_train = CM_train, LOSS_train = LOSS_train, CM_val = CM_val, LOSS_val = LOSS_val)\n","\n","        #gets only the accuracy metric\n","        metrics = getMetrics(CM_val)\n","\n","        MCC_val = metrics[\"mcc\"]\n","        ACC_val = metrics[\"accuracy\"]\n","\n","        t_end = time.time()\n","        t = t_end - t_start\n","\n","        configs.append( ( str(hp), maMax(MCC_val)[1] , maMax(ACC_val)[1] , formatTime(t) ) ) \n","\n","        #sorts list by val accuracy\n","        configs.sort(key = get2ele, reverse = True)\n","\n","        #saves configs to a text file\n","        configs2file( os.path.join(Buffer_dir, \"configs.txt\") , configs)\n","\n","def hyperparameterGridSearch(batchsize, getTrainTransform, getValTransform, getModel, getOptimizer, learning_rate, epochs, \n","                             weight_decay, momentum, color, n_crops):\n","\n","    #adds the parameter lists to a list\n","    parameters = [batchsize, getTrainTransform, getValTransform, getModel, getOptimizer, learning_rate, epochs, weight_decay, \n","                  momentum, color, n_crops]\n","\n","    #gets all of the different combinations of the parameters\n","    hps = getParamCombinations(parameters)\n","    \n","    #prints information\n","    print(f\"Number of combinations: {len(hps)}\\n\")\n","    \n","    hyperparameterSetSearch(hps)\n","\n","def printConfigsFile():\n","  path = os.path.join(Buffer_dir,\"configs.txt\")\n","  f = open(path)\n","  print(f.read())\n","  f.close()\n","\n","def printCurrentConfig(model):\n","  path = os.path.join(Buffer_dir,\"configs.txt\")\n","  f = open(path)\n","\n","  for line in f:\n","    if(line != \"\\n\"):\n","      split1 = line.split(\":\")\n","      split2 = split1[1].split(\",\")[0].strip()\n","      if model == split2:\n","        print(line, end =\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xL9rg6sjAqZg"},"source":["def CM_get_elements(CM):\n","    \n","  TN = CM[:,0,0]\n","  FP = CM[:,0,1]\n","  FN = CM[:,1,0]\n","  TP = CM[:,1,1]\n","  \n","  return TN, FP, FN, TP\n","\n","def CM_get_normalized_elements(CM):\n","  \n","  TN, FP, FN, TP = CM_get_elements(CM)\n","\n","  #Number of real positives in the data\n","  P = TP + FN\n","  #number of real negatives in the data\n","  N = TN + FP\n","\n","  TN = TN/N\n","  FP = FP/N\n","  FN = FN/P\n","  TP = TP/P\n","\n","  return TN, FP, FN, TP\n","\n","#caculates accuracy, precision and recall from confusion matrix\n","def getMetrics(CM):\n","\n","  TN, FP, FN, TP =  CM_get_elements(CM)\n","    \n","  #accuracy\n","  ACC = (TP + TN) / (TP + TN + FP + FN)\n","  #Precision\n","  P = TP / (TP + FP)\n","  #Recall\n","  R = TP / (TP + FN)\n","  #F1 score\n","  F1 = 2 * ((P*R)/(P + R))\n","  #Matthews correlation coefficient\n","  MCC = ((TP * TN) - (FP * FN))/np.sqrt( (TP + FP)*(TP + FN)*(TN + FP)*(TN + FN) )\n","  \n","  #adds metrics to a dictionary\n","  metrics = { \"accuracy\":ACC, \"precision\":P, \"recall\":R, \"f1\":F1, \"mcc\":MCC }\n","\n","  return metrics\n","\n","#calculates the moving average of a vector\n","def movingAverage(vec):\n","    \n","    new_vec = np.zeros(len(vec))\n","    \n","    new_vec[0] = (2*vec[0] + vec[1])/3\n","    new_vec[-1] = (2*vec[-1] + vec[-2])/3\n","    \n","    for i in range(1, len(vec) - 1):\n","        new_vec[i] = (vec[i-1] + vec[i] + vec[i + 1])/3\n","    \n","    return new_vec\n","\n","#prints the max of the moving average\n","def maMax(vec):\n","\n","    ma_vec = movingAverage(vec)\n","\n","    maMax_i = np.argmax(ma_vec)\n","    maMax_value = ma_vec[maMax_i]\n","\n","    return (maMax_i + 1) , maMax_value\n","    \n","def plotMetrics(curves, labels, name_y, ymin = None, ymax = None, moving_average = True):\n","  \n","  epochs = np.arange(1, len(curves[0]) + 1)\n","  \n","  if moving_average:\n","    for i in range(len(curves)):\n","        curves[i] = movingAverage(curves[i])\n","\n","  fig, ax = plt.subplots()\n","  ax.grid()\n","  if ( (ymin != None) and (ymax != None) ):\n","    ax.set_ylim([ymin,ymax])\n","  ax.set_xlabel(\"Epochs\")\n","  ax.set_title(name_y)\n","\n","  for c,label in zip(curves,labels):\n","    ax.plot(epochs, c, label = label)\n","\n","  plt.legend()\n","  plt.tight_layout()\n","  plt.show()\n","\n","def plotConfusionMatrix(CM_train, CM_val, mv = True):\n","\n","  TN_train, FP_train, FN_train, TP_train = CM_get_normalized_elements(CM_train)\n","  TN_val, FP_val, FN_val, TP_val = CM_get_normalized_elements(CM_val)\n","\n","  metrics = [[TN_train,TN_val,\"TN\"], [FP_train, FP_val,\"FP\"], [FN_train, FN_val, \"FN\"],[TP_train, TP_val, \"TP\"]]\n","\n","  for i in range(len(metrics)):\n","    metrics[i][0] = movingAverage(metrics[i][0])\n","    metrics[i][1] = movingAverage(metrics[i][1])\n","\n","  epochs = np.arange(1 , CM_train.shape[0] + 1 )\n","  \n","  fig, axes = plt.subplots(2,2)\n","  axes = axes.flatten()\n","\n","  axes[0].set_ylim([0.5,1])\n","  axes[1].set_ylim([0,0.5])\n","  axes[2].set_ylim([0,0.5])\n","  axes[3].set_ylim([0.5,1])\n","  \n","  fig.suptitle(\"Confusion matrix\")\n","  for metric, ax in zip(metrics,axes):\n","\n","    ax.grid()\n","    ax.set_xlabel(\"Epochs\")\n","    ax.set_title(metric[2])\n","    \n","    ax.plot(epochs,metric[0],label = \"Train\")\n","    ax.plot(epochs,metric[1],label = \"Val\")\n","\n","  plt.legend()\n","  plt.tight_layout()\n","  plt.show()\n","\n","def plotAcc_kfoldVal(kfold_metrics):\n","\n","    fig, (axTrain, axVal) = plt.subplots(2)\n","\n","    axTrain.set_title(\"Train accurary\")\n","    axVal.set_title(\"Validation accurary\")\n","\n","    for i, kfold_metric in enumerate(kfold_metrics):\n","\n","        [ (CM_train, LOSS_train), (CM_val,LOSS_val) ] = kfold_metric\n","\n","        ACC_train = getMetrics(CM_train)[\"accuracy\"]\n","        ACC_val = getMetrics(CM_val)[\"accuracy\"]\n","\n","        axTrain.plot(movingAverage(ACC_train), label = f\"fold {i}\")\n","        axVal.plot(movingAverage(ACC_val), label = f\"fold {i}\")\n","    \n","    plt.tight_layout()\n","\n","    plt.show()\n","\n","def displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val):\n","\n","    val_acc = getMetrics(CM_val)[\"accuracy\"]\n","\n","    print(f\"Maximum validation accuracy: {maMax(val_acc)[1]:.2f} at epoch {maMax(val_acc)[0]}\")\n","\n","    #gets all metrics\n","    trainMetrics = getMetrics(CM_train)\n","    valMetrics = getMetrics(CM_val)\n","    \n","    #extracts individual metrics\n","    ACC_train = trainMetrics[\"accuracy\"]\n","    P_train = trainMetrics[\"precision\"]\n","    R_train = trainMetrics[\"recall\"]\n","    MCC_train = trainMetrics[\"mcc\"]\n","    \n","    ACC_val = valMetrics[\"accuracy\"]\n","    P_val = valMetrics[\"precision\"]\n","    R_val = valMetrics[\"recall\"]\n","    MCC_val = valMetrics[\"mcc\"]\n","\n","    #difference between validation loss and training loss\n","    LOSS_diff = LOSS_val - LOSS_train\n","\n","    plotMetrics([MCC_train, MCC_val], [\"Train\", \"Val\"],\"MCC\", ymin = 0, ymax = 1) #MCC\n","    plotMetrics([ACC_train,ACC_val],[\"Train\", \"Val\"],\"Accuracy\",ymin = 0.5, ymax = 1) # accuracy\n","    plotMetrics([LOSS_train,LOSS_val],[\"Train\", \"Val\"],\"Loss\") #loss\n","    plotMetrics([LOSS_diff],[\"Loss differenece\"],\"Loss differenece\") #loss difference\n","    print(f\"Area under loss difference curve: {LOSS_diff.sum():.2f}\")\n","    plotMetrics([P_train,P_val],[\"Train\", \"Val\"],\"Precision\",ymin = 0.5, ymax = 1) #Precision\n","    plotMetrics([R_train,R_val],[\"Train\", \"Val\"],\"Recall\",ymin = 0.5, ymax = 1) #Recall\n","    plotConfusionMatrix(CM_train, CM_val)\n","\n","def formatTime(seconds):\n","\n","    hours = np.floor( seconds / (60*60) )\n","    seconds_left = seconds % (60*60)\n","    minutes = np.floor(seconds_left / 60)\n","    seconds = seconds % 60\n","\n","    return (f\"{hours:.0f}h {minutes:.0f}m {seconds:.2f}s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzJQROfiAbAS"},"source":["def getImgs(device, loader):\n","\n","  denormalize = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n","                                                    std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n","                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n","                                                    std = [ 1., 1., 1. ]),\n","                              ])\n","\n","  dataset = loader.dataset\n","\n","  #indexes = np.random.randint( len(dataset), size = 4 )\n","  indexes = [1,10,20,30]\n","  imgs = []\n","  labels = []\n","\n","  for i in indexes:\n","    sample = dataset[i]\n","    img = sample[\"img\"][0]\n","\n","    labels.append(sample[\"label\"])\n","\n","    torch_img = denormalize(img).to(device)\n","    normed_torch_img = img.view(1,3,224,224).to(device)\n","\n","    imgs.append( (torch_img,normed_torch_img) )\n","  \n","  return imgs, labels\n","\n","def grad_cam2(indexes, device, loader, model, model_type, layer_name):\n","\n","  denormalize = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n","                                                      std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n","                                  transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n","                                                      std = [ 1., 1., 1. ]),\n","                                ])\n","\n","  dataset = loader.dataset\n","\n","  imgs = []\n","  labels = []\n","\n","  for i in indexes:\n","    sample = dataset[i]\n","    img = sample[\"img\"][0]\n","\n","    labels.append(sample[\"label\"])\n","\n","    torch_img = denormalize(img).to(device)\n","    normed_torch_img = img.view(1,3,224,224).to(device)\n","\n","    imgs.append( (torch_img,normed_torch_img) )\n","\n","  configs = [\n","      dict(model_type=model_type, arch=model, layer_name=layer_name)\n","  ]\n","\n","  for config in configs:\n","      config['arch'].to(device).eval()\n","\n","  cams = [\n","      [cls.from_config(**config) for cls in (GradCAM, GradCAMpp)]\n","      for config in configs\n","  ]\n","\n","  results = []\n","  for gradcam, gradcam_pp in cams:\n","    for torch_img, normed_torch_img in imgs:\n","      mask, _ = gradcam(normed_torch_img)\n","      heatmap, result = visualize_cam(mask, torch_img)\n","\n","      results.append(result)\n","\n","  fig, axes = plt.subplots( len(imgs), 2, figsize = (15,30))\n","\n","  for i in range(len(imgs)):\n","\n","    original_img = transforms.ToPILImage()( imgs[i][0].cpu() )\n","    result = transforms.ToPILImage()(results[i])\n","    axes[i][0].imshow(original_img, aspect='auto')\n","    axes[i][1].imshow(result,aspect='auto')\n","    \n","    if labels[i] == 0:\n","      correct_class = \"Non COVID\"\n","    else:\n","      correct_class = \"COVID\"\n","\n","    normed_torch_img = imgs[i][1]\n","    output = model(normed_torch_img).argmax(dim = 1)\n","    \n","    if output == 0:\n","      predicted_class = \"Non COVID\"\n","    else:\n","      predicted_class = \"COVID\"\n","\n","    axes[i][0].set_title(\"Bonne classe: \" + correct_class, fontsize = 25)\n","    axes[i][1].set_title(\"Classe estimée: \" + predicted_class, fontsize = 25)\n","\n","    axes[i][0].axis(\"off\")\n","    axes[i][1].axis(\"off\")\n","\n","  plt.subplots_adjust(wspace=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyFd-gbqBnL0"},"source":["def grad_cam(device, loader, trained_model):\n","\n","  #turns requires_grad on for all of the layers\n","  for params in trained_model.parameters():\n","    params.requires_grad = True\n","\n","  class densenet_grad_cam(nn.Module):\n","    def __init__(self, trained_model):\n","      super(densenet_grad_cam, self).__init__()\n","      \n","      # get the trained network\n","      self.densenet = trained_model\n","      \n","      # disect the network to access its last convolutional layer\n","      self.features_conv = self.densenet.features\n","      \n","      # add the average global pool\n","      self.global_avg_pool = nn.AvgPool2d(kernel_size=7, stride=1)\n","      \n","      # get the classifier of the vgg19\n","      self.classifier = self.densenet.classifier\n","      \n","      # placeholder for the gradients\n","      self.gradients = None\n","    \n","    # hook for the gradients of the activations\n","    def activations_hook(self, grad):\n","      self.gradients = grad\n","        \n","    def forward(self, x):\n","      x = self.features_conv(x)\n","      \n","      # register the hook\n","      h = x.register_hook(self.activations_hook)\n","      \n","      # don't forget the pooling\n","      x = self.global_avg_pool(x)\n","      x = x.view((1, 1664))\n","      x = self.classifier(x)\n","      return x\n","    \n","    def get_activations_gradient(self):\n","      return self.gradients\n","    \n","    def get_activations(self, x):\n","      return self.features_conv(x)\n","\n","  #geting images\n","  imgs, labels = getImgs(\"cuda\", loader)\n","\n","  results = []\n","  for torch_img, normed_torch_img in imgs:\n","\n","    model = densenet_grad_cam(trained_model = trained_model).to(device)\n","    # set the evaluation mode\n","    model.eval()\n","\n","    torch_img = torch_img.cpu().permute(1,2,0).numpy()\n","\n","    # get the most likely prediction of the model\n","    scores = model(normed_torch_img)\n","    pred = int(scores.argmax(dim = 1))\n","\n","    # get the gradient of the output with respect to the parameters of the model\n","    scores[:, pred].backward()\n","\n","    # pull the gradients out of the model\n","    gradients = model.get_activations_gradient()\n","\n","    # pool the gradients across the channels\n","    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n","\n","    # get the activations of the last convolutional layer\n","    activations = model.get_activations(normed_torch_img).detach()\n","\n","    # weight the channels by corresponding gradients\n","    for i in range(512):\n","        activations[:, i, :, :] *= pooled_gradients[i]\n","        \n","    # average the channels of the activations\n","    heatmap = torch.mean(activations, dim=1).squeeze().cpu().numpy()\n","\n","    # relu on top of the heatmap\n","    # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n","    heatmap = np.maximum(heatmap, 0)\n","\n","    # normalize the heatmap\n","    heatmap /= np.max(heatmap)\n","\n","    heatmap = cv2.resize(heatmap, torch_img.shape[0:2])\n","\n","    heatmap = np.uint8(255 * heatmap)\n","    torch_img = np.uint8(255 * torch_img)\n","\n","    heatmap = cv2.cvtColor(cv2.applyColorMap(heatmap, cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)\n","\n","    superimposed_img = 0.4* heatmap + torch_img\n","    superimposed_img /= np.max(superimposed_img)\n","\n","    results.append(superimposed_img)\n","\n","  fig, axes = plt.subplots( len(imgs), 2, figsize = (15,30))\n","\n","  for i in range(len(imgs)):\n","    original_img = transforms.ToPILImage()( imgs[i][0].cpu() )\n","    axes[i][0].imshow(original_img, aspect='auto')\n","    axes[i][1].imshow(results[i],aspect='auto')\n","    \n","    if labels[i] == 0:\n","      correct_class = \"Non COVID\"\n","    else:\n","      correct_class = \"COVID\"\n","\n","    normed_torch_img = imgs[i][1]\n","    output = model(normed_torch_img).argmax(dim = 1)\n","    \n","    if output == 0:\n","      predicted_class = \"Non COVID\"\n","    else:\n","      predicted_class = \"COVID\"\n","\n","    axes[i][0].set_title(\"Bonne classe: \" + correct_class, fontsize = 25)\n","    axes[i][1].set_title(\"Classe estimée: \" + predicted_class, fontsize = 25)\n","\n","    axes[i][0].axis(\"off\")\n","    axes[i][1].axis(\"off\")\n","\n","  plt.subplots_adjust(wspace=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7kkAkWC9sYh"},"source":["Hold out validation call"]},{"cell_type":"code","metadata":{"id":"ztwDAnBZSpxv"},"source":["train_loader, test_loader = getDataloaders(getTrainTransform_rgb()[0], getValTransform_rgb()[0], batchsize = 16, color = \"RGB\", \n","                                           n_crops = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjHi6xh7-rWE"},"source":["#gets an instance of the model\n","getVGG16 = setUpGetVgg16(unfrozenLayers = 4, classifier_widths=[100])\n","vgg16, modelname = getVGG16()\n","\n","CM_train, LOSS_train, CM_val, LOSS_val = completeTrain(\"cuda\", vgg16, getRMSprop, train_loader, test_loader, epochs = 5, learning_rate = 0.00001, weight_decay = 0.01, momentum = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5i1-Q8nR8yl"},"source":["displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vu-pNtpB_ZZx"},"source":["#gets an instance of the model\n","getResnet = setUpGetResnet50(unfrozenLayers = 4, classifier_widths=[256])\n","resnet, modelname = getResnet()\n","\n","CM_train, LOSS_train, CM_val, LOSS_val = completeTrain(\"cuda\", resnet, getRMSprop, train_loader, test_loader, epochs = 12, learning_rate = 0.00001, weight_decay = 0.01, momentum = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPVl75Fq_fgf"},"source":["displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywcJBEGD_hFE"},"source":["#gets an instance of the model\n","getDensenet = setUpGetDensenet169(unfrozenLayers = 4, classifier_widths=[200])\n","densenet, modelname = getDensenet()\n","\n","CM_train, LOSS_train, CM_val, LOSS_val = completeTrain(\"cuda\", densenet, getRMSprop, train_loader, test_loader, epochs = 10, learning_rate = 0.0001, weight_decay = 0.001, momentum = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OveVHzrHAhaw"},"source":["displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23GTBY1uaUe2"},"source":["indexes = [33,58,61,56]\n","\n","grad_cam2(indexes,\"cuda\", test_loader, vgg16, model_type = \"vgg\", layer_name = \"features_29\")\n","grad_cam2(indexes,\"cuda\", test_loader, resnet, model_type = \"resnet\", layer_name = \"layer4\")\n","grad_cam2(indexes,\"cuda\", test_loader, densenet, model_type = \"densenet\", layer_name = \"features_norm5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLFWFeHZ9oTg"},"source":["Kfold validation call"]},{"cell_type":"code","metadata":{"id":"TTf9LZLTtYQH"},"source":["trainAndValLoaders, test_loader = kfold_getDataloaders(getTrainTransform_rgb()[0], getTrainTransform_rgb()[0], trainVal_test_split = 0.9, k = 5, \n","                                                       batchsize = 16, n_crops = 3)\n","\n","CM_train, LOSS_train, CM_val, LOSS_val = kfold_completeTrain(\"cuda\", trainAndValLoaders, getVgg16, getRMSprop, epochs = 15, learning_rate = 0.00001, \n","                                                             weight_decay = 0.01, momentum = 0 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2r5vF2czsRh"},"source":["displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDO_6rtkztvM"},"source":["hyperparameter grid search"]},{"cell_type":"code","metadata":{"id":"htv49pJ99ZAb"},"source":["getResnets = []\n","\n","getResnets.append(setUpGetResnet50(unfrozenLayers=2, classifier_widths=[256]))\n","\n","hyperparameterGridSearch(batchsize = [16],\n","                         getTrainTransform = [getTrainTransform_rgb],\n","                         getValTransform = [getTrainTransform_rgb],\n","                         getModel = getResnets,\n","                         getOptimizer = [getRMSprop],\n","                         learning_rate = [0.00001], \n","                         epochs = [30],\n","                         weight_decay = [0.001], \n","                         momentum = [0],\n","                         color = [\"RGB\"],\n","                         n_crops = [3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZ4jDAKqzNoL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aji0EdhLu6B"},"source":["Hyperparameter set search"]},{"cell_type":"code","metadata":{"id":"qNi-q3TILtmJ"},"source":["hp1 = hyperparameter(batchsize = 16, \n","                     getTrainTransform = getTrainTransform_rgb, \n","                     getValTransform = getTrainTransform_rgb, \n","                     getModel = setUpGetDensenet169(unfrozenLayers=4, classifier_widths=[128]),\n","                     getOptimizer = getRMSprop,\n","                     learning_rate = 0.0001,\n","                     epochs = 30,\n","                     weight_decay = 0.001,\n","                     momentum = 0,\n","                     color = \"RGB\",\n","                     n_crops = 3)\n","\n","hyperparameters = [hp1]\n","\n","hyperparameterSetSearch(hyperparameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TsuhODMxzLIt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8uEGW8I7qys"},"source":["\n","Displaying metrics saved to file"]},{"cell_type":"code","metadata":{"id":"G9pV-pZNmBds"},"source":["printConfigsFile()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDFtWErH7FoJ"},"source":["name = \"Model 13\"\n","\n","CM_train, LOSS_train, CM_val, LOSS_val = loadNpz(name)\n","\n","printCurrentConfig(name)\n","displayAllMetrics(CM_train, LOSS_train, CM_val, LOSS_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABUTXRXKY_5t"},"source":["p = 24\n","\n","Metrics = getMetrics(CM_val)\n","\n","acc = Metrics[\"accuracy\"][p]\n","mcc = Metrics[\"mcc\"][p]\n","precision = Metrics[\"precision\"][p]\n","recall = Metrics[\"recall\"][p]\n","\n","print(f\"acc: {acc:.2f}\")\n","print(f\"mcc: {mcc:.2f}\")\n","print(f\"precision: {precision:.2f}\")\n","print(f\"recall: {recall:.2f}\")\n","\n","TN, FP, FN, TP = CM_get_normalized_elements(CM_val)\n","\n","print()\n","print(f\"TN:{TN[p]:.2f}\")\n","print(f\"FP:{FP[p]:.2f}\")\n","print(f\"FN:{FN[p]:.2f}\")\n","print(f\"TP:{TP[p]:.2f}\")\n","\n","print()\n","print(f\"Loss: {LOSS_val[p]:.2f}\")\n","print(f\"Loss diff: {LOSS_val[p] - LOSS_train[p]:.2f}\")"],"execution_count":null,"outputs":[]}]}